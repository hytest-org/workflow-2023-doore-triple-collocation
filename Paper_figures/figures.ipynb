{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdbd626-0771-4d83-b2a4-7b178e66afcd",
   "metadata": {},
   "source": [
    "# Figures for [Doore et al. (2025)]()\n",
    "\n",
    "This is the code to replicate the figures in [Doore et al. (2025)]().\n",
    "Additionally, it creates the LaTeX tables and prints out the quoted statistics.\n",
    "To run, it requires the whole workflow to have been ran first as it depends on some output files from the workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d83b3-d84f-4743-b887-6cd2d0fbab1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from xarray_einstats import linalg, stats\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from shapely import unary_union, box\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import cartopy\n",
    "from cartopy import crs as ccrs, feature as cfeature\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694b0ff-b7f4-451e-a91c-bedb1dc9b19b",
   "metadata": {},
   "source": [
    "Read in and group the [regridded data sets](../workflow/1_regrid.ipynb) for easy loop style plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228f2d8-1d8d-4593-a5e6-18ee112cb1d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssebop = xr.open_dataset('../Data/ssebop/ssebop_aet_regridded.nc', engine='netcdf4')\n",
    "gleam = xr.open_dataset('../Data/gleam/gleam_aet.nc', engine='netcdf4')\n",
    "era5 = xr.open_dataset('../Data/era5/era5_aet_regridded.nc', engine='netcdf4')\n",
    "nldas = xr.open_dataset('../Data/nldas/nldas_aet_regridded.nc', engine='netcdf4')\n",
    "terra = xr.open_dataset('../Data/terraclimate/terraclimate_aet_regridded.nc')\n",
    "wbet = xr.open_dataset('../Data/wbet/wbet_aet_regridded.nc', engine='netcdf4')\n",
    "\n",
    "datasets = xr.concat([ssebop, gleam, era5, nldas, terra, wbet],\n",
    "                     dim=pd.Index(['SSEBop', 'GLEAM', 'ERA5', 'NLDAS', 'TerraClimate', 'WBET'],\n",
    "                                  name='dataset_name')).compute()\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022d84c-90c6-4ca2-a301-42b008df2922",
   "metadata": {},
   "source": [
    "As we will be plotting CONUS, let's make a mask of CONUS for all data sets. Since WBET is limited to CONUS, we can just use its data to create the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22674742-a308-4eb9-84da-e204aaadb640",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the mean summer data as it ensures no NaNs in CONUS\n",
    "conus_mask = (datasets\n",
    "              .sel(dataset_name='WBET',\n",
    "                   time=(datasets.time.dt.season == 'JJA'))\n",
    "              .mean(dim='time')\n",
    "              .drop_vars('dataset_name')\n",
    "              .aet)\n",
    "# Normalize it\n",
    "conus_mask = conus_mask/conus_mask\n",
    "conus_mask.name = 'conus_mask'\n",
    "conus_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa20995-2197-4e06-a606-765cdd94552a",
   "metadata": {},
   "source": [
    "## Figure 1\n",
    "\n",
    "The graphical depiction of the TC affine error model. This is in the `TC/TC_diagram/` folder for use in the TC description. So, let's just run that notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4779692-6b75-44bf-826e-1d8cd9f5e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/TC_diagram/TC_figure.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21e586-13b2-46bf-bd30-bc44d74e5972",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 2\n",
    "\n",
    "Map the mean summer ET of each data set at GLEAM resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92e52b-d0ed-40a0-814b-bd8cd26ee710",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "projPC = ccrs.PlateCarree()\n",
    "# projPC = ccrs.epsg(5070)\n",
    "letter = 'abcdef'\n",
    "\n",
    "data = datasets.sel(time=(datasets.time.dt.season == 'JJA')).mean(dim='time') * conus_mask\n",
    "\n",
    "# Define the figure and each axis for the 2 rows and 3 columns\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3,\n",
    "                         subplot_kw={'projection': projPC},\n",
    "                         figsize=(15, 5))\n",
    "\n",
    "# axs is a 2 dimensional array of `GeoAxes`.  We will flatten it into a 1-D array\n",
    "ax = ax.flatten()\n",
    "\n",
    "#Loop over all of the datasets\n",
    "for i, dataset_name in enumerate(data.dataset_name.data):\n",
    "    \n",
    "    ax[i].set_extent([-126, -66, 24, 52], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Select the specified dataset\n",
    "    data_plt = data.aet.sel(dataset_name=dataset_name)\n",
    "\n",
    "    # Map plot\n",
    "    cs = data_plt.plot(ax=ax[i], transform=ccrs.PlateCarree(),\n",
    "                       add_colorbar=False, vmin=0, vmax=160,\n",
    "                       rasterized=True, cmap='plasma')\n",
    "\n",
    "\n",
    "    # Title each subplot with the name of the dataset\n",
    "    ax[i].set_title('(' + letter[i] + ') ' + dataset_name,\n",
    "                    fontdict={'fontsize': 15})\n",
    "\n",
    "    # Draw the coastines for each subplot\n",
    "    ax[i].coastlines(resolution='110m', rasterized=True)\n",
    "    ax[i].add_feature(cfeature.STATES, rasterized=True)\n",
    "\n",
    "    # Create the tick labels on the outside plots\n",
    "    draw_labels = {}\n",
    "    if i > 2:\n",
    "        draw_labels['bottom'] = 'x'\n",
    "    if i == 0 or i == 3:\n",
    "        draw_labels['left'] = 'y'\n",
    "    ax[i].gridlines(draw_labels=draw_labels, alpha=0,\n",
    "                    xlabel_style={'size': 14}, ylabel_style={'size': 14})\n",
    "    \n",
    "    # Add tick marks to the labels\n",
    "    xticks = range(-120, -60, 10)\n",
    "    ax[i].set_xticks(xticks, crs=projPC)\n",
    "    ax[i].set_xticklabels(['' for i in range(len(xticks))])\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].tick_params(axis=\"x\", direction=\"in\", bottom=True, top=True)\n",
    "\n",
    "    yticks = range(25, 50, 5)\n",
    "    ax[i].set_yticks(yticks, crs=projPC)\n",
    "    ax[i].set_yticklabels(['' for i in range(len(yticks))])\n",
    "    ax[i].set_ylabel('')\n",
    "    ax[i].tick_params(axis=\"y\", direction=\"in\", right=True, left=True)\n",
    "\n",
    "\n",
    "# Add a colorbar axis at the side of the graph\n",
    "cbar_ax = fig.add_axes([0.925, 0.11, 0.01, 0.82])\n",
    "cbar_ax.tick_params(axis=\"y\", direction=\"in\")\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar=fig.colorbar(cs, cax=cbar_ax, extend='max')\n",
    "cbar.set_label('Evapotranspiration\\n[mm month$^{-1}$]', fontsize=15)\n",
    "cbar.ax.tick_params(labelsize=14) \n",
    "\n",
    "fig.supxlabel('Longitude', y=0.005, fontsize=15)\n",
    "fig.supylabel('Latitude', x=0.005, fontsize=15)\n",
    "# Adjust the location of the subplots on the page to make room for the colorbar\n",
    "fig.subplots_adjust(left=0.07, right=0.91, bottom=0.1, top=0.95, wspace=0.02, hspace=0.15)\n",
    "\n",
    "fig.savefig('mean_summer_ET.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fc165-dd9f-4106-b0f4-1fb462323d0f",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "\n",
    "Plot the regionally aggregated time series for the three regions. Add an inset of the location of each region in the US in the upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa605715-4446-4638-984a-125bea81b140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we need to create the weight map to do the spatial aggregation\n",
    "# See the Regional Analysis notebook for more details on this code block\n",
    "def grid_to_poly_boxes(grid, lat_coord='latitude', lon_coord='longitude'):\n",
    "    lat_diff = grid[lat_coord].diff(dim=lat_coord)\n",
    "    lat_spacing = np.abs(np.unique(lat_diff))\n",
    "        \n",
    "    lon_diff = grid[lon_coord].diff(dim=lon_coord)\n",
    "    lon_spacing = np.abs(np.unique(lon_diff))\n",
    "        \n",
    "    bounds = np.vstack((grid[lat_coord] + lat_spacing/2,\n",
    "                        grid[lat_coord] - lat_spacing/2)).T\n",
    "    grid[lat_coord+'_bounds'] = ((lat_coord, 'bound'), bounds)\n",
    "    bounds = np.vstack((grid[lon_coord] + lon_spacing/2,\n",
    "                        grid[lon_coord] - lon_spacing/2)).T\n",
    "    grid[lon_coord+'_bounds'] = ((lon_coord, 'bound'), bounds)\n",
    "\n",
    "    points = grid.stack(point=(lat_coord, lon_coord))\n",
    "    \n",
    "    def bounds_to_poly(lat_bounds, lon_bounds):\n",
    "        if lon_bounds[0] >= 180:\n",
    "            lon_bounds = lon_bounds - 360\n",
    "        return box(lon_bounds[0],\n",
    "                   lat_bounds[0],\n",
    "                   lon_bounds[1],\n",
    "                   lat_bounds[1])\n",
    "\n",
    "    boxes = xr.apply_ufunc(\n",
    "        bounds_to_poly,\n",
    "        points[lat_coord+'_bounds'],\n",
    "        points[lon_coord+'_bounds'],\n",
    "        input_core_dims=[('bound',),  ('bound',)],\n",
    "        output_dtypes=[np.dtype('O')],\n",
    "        vectorize=True\n",
    "    )\n",
    "\n",
    "    return boxes\n",
    "\n",
    "grid = datasets[['lat', 'lon']]\n",
    "boxes = grid_to_poly_boxes(grid, lat_coord='lat', lon_coord='lon')\n",
    "\n",
    "grid_df= gpd.GeoDataFrame(\n",
    "    data={'geometry': boxes.data, 'lat': boxes.lat, 'lon': boxes.lon},\n",
    "    index=boxes.indexes['point'],\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "high_plns_aqfr = gpd.read_file('../Data/regions/High_Plains_aquifer.zip')\n",
    "high_plns_aqfr = high_plns_aqfr[high_plns_aqfr['AQUIFER'] == 'High Plains aquifer']\n",
    "high_plns_aqfr = gpd.GeoDataFrame(geometry=[unary_union(high_plns_aqfr.geometry.values)],\n",
    "                                  crs=high_plns_aqfr.crs)\n",
    "high_plns_aqfr = high_plns_aqfr.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "high_plns_aqfr['region_name'] = 'High Plains Aquifer'\n",
    "\n",
    "cntrl_valley = gpd.read_file('../Data/regions/Central_Valley.zip')\n",
    "cntrl_valley = gpd.GeoDataFrame(geometry=[unary_union(cntrl_valley.geometry.buffer(0.001).values)],\n",
    "                                crs=cntrl_valley.crs)\n",
    "cntrl_valley = cntrl_valley.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "cntrl_valley['region_name'] = 'Central Valley'\n",
    "\n",
    "ucrb = gpd.read_file('../Data/regions/Upper_Colorado_River_Basin.zip')\n",
    "ucrb = ucrb.drop(columns=['EXT_ID', 'EXT_TYP_ID', 'NAME'])\n",
    "ucrb = ucrb.to_crs('EPSG:4326')\n",
    "ucrb['region_name'] = 'Upper Colorado River Basin'\n",
    "\n",
    "regions = pd.concat([high_plns_aqfr, cntrl_valley, ucrb], ignore_index=True)\n",
    "\n",
    "overlay = grid_df.to_crs('EPSG:5070').overlay(regions.to_crs('EPSG:5070'))\n",
    "\n",
    "grid_cell_fraction = (\n",
    "    overlay.geometry.area.groupby(overlay['region_name'])\n",
    "    .transform(lambda x: x / x.sum())\n",
    ")\n",
    "\n",
    "multi_index = overlay.set_index(['lat', 'lon', 'region_name']).index\n",
    "df_weights = pd.DataFrame({'weights': grid_cell_fraction.values}, index=multi_index)\n",
    "da_weights = xr.Dataset(df_weights).unstack(fill_value=0).weights\n",
    "da_weights, _ = xr.align(da_weights, datasets, join='outer',\n",
    "                         exclude=['time', 'dataset_name'],\n",
    "                         fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7b905-9400-46a0-8420-ce65e94e17bc",
   "metadata": {},
   "source": [
    "Now plot the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537459d-0756-43a3-a3b0-2d54ca0a920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_weighted = datasets.weighted(da_weights)\n",
    "ds_et_regional = et_weighted.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "ds_et_regional = ds_et_regional.sel(time=slice('2003-01', '2018-09'))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 4), sharey=True)\n",
    "    \n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']\n",
    "handles = []\n",
    "for i, dataset_name in enumerate(ds_et_regional.dataset_name.data):\n",
    "    handles.append(mlines.Line2D([], [], label=dataset_name, color=f'tab:{colors[i]}'))\n",
    "\n",
    "for i, region_name in enumerate(ds_et_regional.region_name.data):\n",
    "    ds_et_regional.aet.sel(region_name=region_name).plot.line(x='time', ax=ax[i], ylim=(-5, 180), add_legend=False)\n",
    "    ax[i].set_xlabel('Year', fontsize=15)\n",
    "    ax[i].xaxis.set_major_locator(mdates.YearLocator(base=3))\n",
    "    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel('Evapotranspiration\\n[mm month$^{-1}$]', fontsize=15)\n",
    "    else:\n",
    "        ax[i].set_ylabel(None)\n",
    "\n",
    "    ax[i].set_title(f'({letter[i]}) {region_name}', fontsize=15)\n",
    "    ax[i].tick_params(axis=\"both\", direction=\"in\", right=True, top=True, labelsize=14)\n",
    "    if i == 2:\n",
    "        ax[i].legend(handles=handles, loc='upper left', fontsize=14)\n",
    "\n",
    "    ax[i].set_ylim(-10, 220)\n",
    "\n",
    "    ax_in = inset_axes(ax[i], width='40%', height='40%', loc=\"upper right\", \n",
    "                       axes_class=cartopy.mpl.geoaxes.GeoAxes, \n",
    "                       axes_kwargs=dict(projection=projPC))\n",
    "\n",
    "    ax_in.set_extent([-126, -66, 24, 52], crs=projPC)\n",
    "    # Draw the coastines and states\n",
    "    ax_in.coastlines(resolution='110m', linewidth=0.4, rasterized=True)\n",
    "    ax_in.add_feature(cfeature.STATES, linewidth=0.4, rasterized=True)\n",
    "\n",
    "    cs = regions[regions['region_name'] == region_name].plot(ax=ax_in, transform=projPC, color='tab:red', rasterized=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('regional_time_series.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eedf9b-45cf-4483-a3f2-28fc80c90a6a",
   "metadata": {},
   "source": [
    "## Figure 4\n",
    "\n",
    "Plot the error cross-correlation estimates of the data sets in a lower triangular corner plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f4824-6e06-41cc-9265-53d40e86c2d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ec_errs = xr.open_dataset('../Data/EC_errs.nc')\n",
    "\n",
    "projPC = ccrs.PlateCarree()\n",
    "letter = np.array([['a', 'b', 'c'],\n",
    "                   ['d', 'e', 'f'],\n",
    "                   ['g', 'h', 'i'],\n",
    "                   ['j', 'k', 'l'],\n",
    "                   ['m', 'n', 'o']]).T.flatten()\n",
    "\n",
    "# Define the figure and each axis for the 2 rows and 3 columns\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5,\n",
    "                         subplot_kw={'projection': projPC},\n",
    "                         figsize=(22, 11))\n",
    "\n",
    "# axs is a 2 dimensional array of `GeoAxes`.  We will flatten it into a 1-D array\n",
    "# ax = ax.T.flatten()\n",
    "\n",
    "#Loop over all of the datasets\n",
    "j = 0\n",
    "i = -1\n",
    "for k, covar_pair in enumerate(ec_errs.covar_pair.data):\n",
    "    i += 1\n",
    "    if k == 5:\n",
    "        i = 1\n",
    "        j += 1\n",
    "    if k == 9:\n",
    "        i = 2\n",
    "        j += 1\n",
    "    if k == 12:\n",
    "        i = 3\n",
    "        j += 1\n",
    "    if k == 14:\n",
    "        i = 4\n",
    "        j += 1\n",
    "    \n",
    "    ax[i, j].set_extent([-126, -66, 24, 52], crs=projPC)\n",
    "\n",
    "    # Select the specified dataset\n",
    "    data = ec_errs.rho.median(dim='est_idx').sel(covar_pair=covar_pair, season='All') * conus_mask\n",
    "\n",
    "    # Map plot\n",
    "    cs = data.plot(ax=ax[i, j], transform=projPC,\n",
    "                   add_colorbar=False, vmin=-1, vmax=1,\n",
    "                   rasterized=True, cmap='PuOr')\n",
    "\n",
    "    # Title each subplot with the name of the dataset\n",
    "    if i == j:\n",
    "        ax[i, j].set_title(covar_pair.split()[0], fontdict={'fontsize': 20})\n",
    "    else:  \n",
    "        ax[i, j].set_title(None)\n",
    "\n",
    "    # Draw the coastines for each subplot\n",
    "    ax[i, j].coastlines(resolution='110m', rasterized=True)\n",
    "    ax[i, j].add_feature(cfeature.STATES, rasterized=True)\n",
    "\n",
    "    # Create the tick labels on the outside plots\n",
    "    draw_labels = {}\n",
    "    if (i % 5) == 4:\n",
    "        draw_labels['bottom'] = 'x'\n",
    "    if k < 5:\n",
    "        draw_labels['left'] = 'y'\n",
    "    ax[i, j].gridlines(draw_labels=draw_labels, alpha=0,\n",
    "                    xlabel_style={'size': 14}, ylabel_style={'size': 14})\n",
    "    \n",
    "    # Add tick marks to the labels\n",
    "    xticks = range(-120, -60, 10)\n",
    "    ax[i, j].set_xticks(xticks, crs=projPC)\n",
    "    ax[i, j].set_xticklabels(['' for tick in range(len(xticks))])\n",
    "    ax[i, j].set_xlabel('')\n",
    "    ax[i, j].tick_params(axis=\"x\", direction=\"in\", bottom=True, top=True)\n",
    "\n",
    "    yticks = range(25, 50, 5)\n",
    "    ax[i, j].set_yticks(yticks, crs=projPC)\n",
    "    ax[i, j].set_yticklabels(['' for tick in range(len(yticks))])\n",
    "    ax[i, j].set_ylabel('')\n",
    "    ax[i, j].tick_params(axis=\"y\", direction=\"in\", right=True, left=True)\n",
    "\n",
    "    if k < 5:\n",
    "        ax[i, j].set_ylabel(f\"{covar_pair.split()[1]}\\n\\n\", fontsize=20)\n",
    "\n",
    "for i in range(0, 4):\n",
    "    for j in range(i+1, 5):\n",
    "        ax[i, j].axis('off')\n",
    "# Add a colorbar axis at the side of the graph\n",
    "cbar_ax = fig.add_axes([0.93, 0.3, 0.01, 0.63])\n",
    "cbar_ax.tick_params(axis=\"y\", direction=\"in\")\n",
    "\n",
    "# Draw the colorbar\n",
    "cbar=fig.colorbar(cs, cax=cbar_ax)\n",
    "cbar.set_label(r'Error Cross-Correlation ($\\rho_{\\varepsilon_i, \\varepsilon_j}$)', fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=18) \n",
    "\n",
    "# fig.supxlabel('Longitude', y=0.005, fontsize=15)\n",
    "# fig.supylabel('Latitude', x=0.005, fontsize=15)\n",
    "# Adjust the location of the subplots on the page to make room for the colorbar\n",
    "fig.subplots_adjust(left=0.055, right=0.99, bottom=0.05, top=0.95, wspace=0.02, hspace=0.02)\n",
    "\n",
    "fig.savefig('median_rho.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9d8e2-3437-46be-b844-fd8849e1a2b6",
   "metadata": {},
   "source": [
    "## Figure 5\n",
    "\n",
    "Create a bar chart for the regional error cross-correlations. Include the 16th and 84th percentile ranges as error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a11fbf-d1db-4ad9-a740-df6f65c7cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "errs = ec_errs.rename({'est_pair': 'est_pair_cov', 'est_idx': 'est_idx_cov'})\n",
    "weighted_results = errs.weighted(da_weights)\n",
    "\n",
    "# Use mean rather than sum as it accounts for NaNs\n",
    "# The are equivalent since the sum of weights is 1\n",
    "regional_errs = weighted_results.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "# Do non-NaN area weighting\n",
    "regional_weights = weighted_results.sum_of_weights(dim=['lat', 'lon'], keep_attrs=True)\n",
    "regional_weights /= regional_weights.sum(dim='est_idx_cov')\n",
    "\n",
    "regional_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc0183-0648-4740-b315-bbfa0d7adabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in ['All']:#regional_errs.season.data:\n",
    "    data = (regional_errs * regional_weights).sel(season=season)\n",
    "    data_err = regional_errs.sel(season=season)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7.2, 4))\n",
    "    \n",
    "    x = np.arange(3)  # the label locations\n",
    "    width = 1/(3 + len(data.covar_pair))  # the width of the bars\n",
    "    multiplier = 2\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0,1,15))\n",
    "    for color, covar_pair in zip(colors, data.covar_pair.data):\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, data.rho.sum(dim='est_idx_cov').sel(covar_pair=covar_pair).data,\n",
    "                          width, label=covar_pair.replace(' ', '-'), color=color, edgecolor='black')\n",
    "        _ = ax.errorbar(x + offset, data.rho.sum(dim='est_idx_cov').sel(covar_pair=covar_pair).data,\n",
    "                           yerr=np.abs(data_err.rho.quantile([0.16, 0.84], dim='est_idx_cov').sel(covar_pair=covar_pair).data\n",
    "                                       - data.rho.sum(dim='est_idx_cov').sel(covar_pair=covar_pair).data),\n",
    "                           linestyle='', color='black', capsize=3)\n",
    "        multiplier += 1\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.plot(ax.get_xlim(), [0, 0], color='black', linestyle='dashed', linewidth=1)\n",
    "    ax.set_ylabel(r'Error Cross-Correlation ($\\rho_{\\varepsilon_i, \\varepsilon_j}$)', fontsize=14)\n",
    "    xticks = data.region_name.data\n",
    "    xticks[2] = 'Upper Colorado\\nRiver Basin'\n",
    "    ax.set_xticks(x + 9 * width, xticks)\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.legend(loc='upper left', ncols=3, columnspacing=1)\n",
    "    ax.set_ylim(-1, 2.5)\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", right=True, top=True, labelsize=12)\n",
    "    ax.set_yticks(np.linspace(-1, 1, 5), np.linspace(-1, 1, 5)) \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if season == 'All':\n",
    "        fig.savefig('regional_rho.pdf')\n",
    "    else:\n",
    "        fig.savefig(f'regional_rho_{season}.pdf')\n",
    "\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe9288-b26a-43b3-8a0c-5355410a282a",
   "metadata": {},
   "source": [
    "## Figure 6 (and Appendix Figures)\n",
    "\n",
    "Plot the agreement probability of the data set pairs using the median covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8e790-5eff-4181-8686-d0193aea6675",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Create the agreement probabilities\n",
    "rel_bias = xr.open_dataset('../Data/avg_bias.nc')\n",
    "ds_results = xr.merge([ec_errs.rename({'covar_pair': 'dataset_pair'}),\n",
    "                       rel_bias])\n",
    "\n",
    "variances = ds_results['covar'].linalg.diagonal(\n",
    "                dims=['covar_pair_idx_1', 'covar_pair_idx_2'], offset=0,\n",
    "            )\n",
    "covariances = ds_results['covar'].sel(covar_pair_idx_1=0, covar_pair_idx_2=1).squeeze()\n",
    "ds_results['sigma_bias'] = np.sqrt(variances.sum(dim='covar_pair_idx_1') - 2 * covariances)\n",
    "\n",
    "norm_dist = stats.XrContinuousRV(norm,\n",
    "                                 loc=np.abs(ds_results['median_bias']),\n",
    "                                 scale=ds_results['sigma_bias'])\n",
    "# Set and name it as a DataArray as the attributes of the coordinates are not kept\n",
    "agreement_probability = norm_dist.cdf(0)\n",
    "agreement_probability.name = 'agreement_probability'\n",
    "\n",
    "# Merge to preserve coordinate attributes\n",
    "ds_results = xr.merge([ds_results, agreement_probability])\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d39d85-7f86-4d5a-9fb4-4354e8749984",
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in ds_results.season.data:\n",
    "    # Define the figure and each axis for the 2 rows and 3 columns\n",
    "    fig, ax = plt.subplots(nrows=5, ncols=5,\n",
    "                           subplot_kw={'projection': projPC},\n",
    "                           figsize=(22, 11))\n",
    "        \n",
    "    #Loop over all of the datasets\n",
    "    j = 0\n",
    "    i = -1\n",
    "    for k, dataset_pair in enumerate(ds_results.dataset_pair.data):\n",
    "        i += 1\n",
    "        if k == 5:\n",
    "            i = 1\n",
    "            j += 1\n",
    "        if k == 9:\n",
    "            i = 2\n",
    "            j += 1\n",
    "        if k == 12:\n",
    "            i = 3\n",
    "            j += 1\n",
    "        if k == 14:\n",
    "            i = 4\n",
    "            j += 1\n",
    "    \n",
    "        ax[i, j].set_extent([-126, -66, 24, 52], crs=projPC)\n",
    "    \n",
    "        # Select the specified dataset\n",
    "        data = ds_results['agreement_probability'].median(dim='est_idx').sel(dataset_pair=dataset_pair, season=season) * conus_mask\n",
    "\n",
    "        # Map plot\n",
    "        cs = data.plot(ax=ax[i, j], transform=projPC,\n",
    "                       add_colorbar=False, vmin=0, vmax=0.5,\n",
    "                       rasterized=True, cmap='plasma')\n",
    "    \n",
    "        # Title each subplot with the name of the dataset\n",
    "        if i == j:\n",
    "            ax[i, j].set_title(dataset_pair.split()[0], fontdict={'fontsize': 20})\n",
    "        else:  \n",
    "            ax[i, j].set_title(None)\n",
    "    \n",
    "        # Draw the coastines for each subplot\n",
    "        ax[i, j].coastlines(resolution='110m', rasterized=True)\n",
    "        ax[i, j].add_feature(cfeature.STATES, rasterized=True)\n",
    "    \n",
    "        # Create the tick labels on the outside plots\n",
    "        draw_labels = {}\n",
    "        if (i % 5) == 4:\n",
    "            draw_labels['bottom'] = 'x'\n",
    "        if k < 5:\n",
    "            draw_labels['left'] = 'y'\n",
    "        ax[i, j].gridlines(draw_labels=draw_labels, alpha=0,\n",
    "                        xlabel_style={'size': 14}, ylabel_style={'size': 14})\n",
    "        \n",
    "        # Add tick marks to the labels\n",
    "        xticks = range(-120, -60, 10)\n",
    "        ax[i, j].set_xticks(xticks, crs=projPC)\n",
    "        ax[i, j].set_xticklabels(['' for tick in range(len(xticks))])\n",
    "        ax[i, j].set_xlabel('')\n",
    "        ax[i, j].tick_params(axis=\"x\", direction=\"in\", bottom=True, top=True)\n",
    "    \n",
    "        yticks = range(25, 50, 5)\n",
    "        ax[i, j].set_yticks(yticks, crs=projPC)\n",
    "        ax[i, j].set_yticklabels(['' for tick in range(len(yticks))])\n",
    "        ax[i, j].set_ylabel('')\n",
    "        ax[i, j].tick_params(axis=\"y\", direction=\"in\", right=True, left=True)\n",
    "    \n",
    "        if k < 5:\n",
    "            ax[i, j].set_ylabel(f\"{dataset_pair.split()[1]}\\n\\n\", fontsize=20)\n",
    "    \n",
    "    for i in range(0, 4):\n",
    "        for j in range(i+1, 5):\n",
    "            ax[i, j].axis('off')\n",
    "    # Add a colorbar axis at the side of the graph\n",
    "    cbar_ax = fig.add_axes([0.93, 0.3, 0.01, 0.63])\n",
    "    cbar_ax.tick_params(axis=\"y\", direction=\"in\")\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar=fig.colorbar(cs, cax=cbar_ax)\n",
    "    cbar.set_label(r'Agreement Probability', fontsize=20)\n",
    "    cbar.ax.tick_params(labelsize=18) \n",
    "\n",
    "    # fig.supxlabel('Longitude', y=0.005, fontsize=15)\n",
    "    # fig.supylabel('Latitude', x=0.005, fontsize=15)\n",
    "    # Adjust the location of the subplots on the page to make room for the colorbar\n",
    "    fig.subplots_adjust(left=0.055, right=0.99, bottom=0.05, top=0.95, wspace=0.02, hspace=0.02)\n",
    "\n",
    "    if season == 'All':\n",
    "        fig.savefig('agreement.pdf')\n",
    "    else:\n",
    "        fig.savefig(f'agreement_{season}.pdf')\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7559525-f565-44aa-9fb7-7ef87862015b",
   "metadata": {},
   "source": [
    "## Table 2\n",
    "\n",
    "Make Table 2 using the agreement probability data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866906c9-40dc-42d4-a09b-352bc3bf615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [r'\\begin{table*}[t]']\n",
    "table.append(r'    \\caption{The percentage of grid cells across CONUS with agreement probabilities '\n",
    "             r'above the given significance levels for each of the data set pairs.}')\n",
    "table.append(r'    \\label{tab:PercCellSigLevel}')\n",
    "table.append(r'    \\footnotesize')\n",
    "table.append(r'    \\begin{tabular}{lrrrrrrrrrrrrrrr}')\n",
    "table.append(r'        \\tophline')\n",
    "table.append(r'        Data Set Pairs & \\multicolumn{3}{c}{Non-seasonal} & '\n",
    "             r'\\multicolumn{3}{c}{Winter} & \\multicolumn{3}{c}{Spring} & '\n",
    "             r'\\multicolumn{3}{c}{Summer} & \\multicolumn{3}{c}{Fall}\\\\')\n",
    "table.append(r'        & \\multicolumn{3}{c}{Significance Levels} & \\multicolumn{3}{c}{Significance Levels} & '\n",
    "             r'\\multicolumn{3}{c}{Significance Levels} & \\multicolumn{3}{c}{Significance Levels} & '\n",
    "             r'\\multicolumn{3}{c}{Significance Levels} \\\\')\n",
    "table.append(r'        & 0.16 & 0.05 & 0.01 & 0.16 & 0.05 & 0.01 & 0.16 & 0.05 & 0.01 & 0.16 & 0.05 & 0.01 & 0.16 & 0.05 & 0.01\\\\')\n",
    "table.append(r'        \\middlehline')\n",
    "\n",
    "for dataset_pair in ds_results.dataset_pair.data:\n",
    "    line = f\"        {dataset_pair.replace(' ', '-')} & \"\n",
    "    for season in ['All', 'DJF', 'MAM', 'JJA', 'SON']:\n",
    "        conus_agree = ds_results['agreement_probability'].sel(season=season, dataset_pair=dataset_pair).median(dim='est_idx') * conus_mask\n",
    "        conus_frac16 = (conus_agree > 0.16).sum(dim=['lat', 'lon']) / conus_mask.sum()\n",
    "        conus_frac05 = (conus_agree > 0.05).sum(dim=['lat', 'lon']) / conus_mask.sum()\n",
    "        conus_frac01 = (conus_agree > 0.01).sum(dim=['lat', 'lon']) / conus_mask.sum()\n",
    "        line += f\"{conus_frac16*100:.1f} & {conus_frac05*100:.1f} & {conus_frac01*100:.1f} \"\n",
    "        if season == 'SON':\n",
    "            line += r'\\\\'\n",
    "        else:\n",
    "            line += '& '\n",
    "\n",
    "        if season == 'All':\n",
    "            print(f'Fraction of {dataset_pair} > 0.16 (0.05): {conus_frac16.data:0.3f} ({conus_frac05.data:0.3f})')\n",
    "\n",
    "    table.append(line)\n",
    "\n",
    "table.append(r'        \\bottomhline')\n",
    "table.append(r'    \\end{tabular}')\n",
    "table.append(r'\\end{table*}')\n",
    "\n",
    "with open('table_PercCellSigLevel.tex', 'w') as outfile:\n",
    "    outfile.writelines((str(i)+'\\n' for i in table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53583220-5cb0-4afb-98db-1a005840b695",
   "metadata": {},
   "source": [
    "Print quoted stats for the seasonal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe20b7-d516-4b02-bbad-43db798b5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in ['DJF', 'MAM', 'JJA', 'SON']:\n",
    "    conus_prob = ds_results['agreement_probability'].sel(season=season).median(dim='est_idx') * conus_mask\n",
    "    conus_frac16 = (conus_prob > 0.16).sum(dim=['lat', 'lon', 'dataset_pair']) / (conus_mask.sum() * len(ds_results.dataset_pair))\n",
    "    conus_frac05 = (conus_prob > 0.05).sum(dim=['lat', 'lon', 'dataset_pair']) / (conus_mask.sum() * len(ds_results.dataset_pair))\n",
    "    print(f'Fraction in {season} with p > 0.16 (0.05): {conus_frac16.data:0.3f} ({conus_frac05.data:0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12618241-785e-4f6c-9986-3ec8b17561fb",
   "metadata": {},
   "source": [
    "# Figure 7\n",
    "\n",
    "Make a bar plot of the agreement across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89036af9-ea6a-4667-871b-fde0a23cc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regional agreement\n",
    "weighted_bias = rel_bias.weighted(da_weights)\n",
    "\n",
    "# Use mean rather than sum as it accounts for NaNs\n",
    "regional_bias = weighted_bias.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "# Revert the error variances back to error standard deviations\n",
    "regional_results = xr.merge([regional_errs.rename({'covar_pair': 'dataset_pair',\n",
    "                                                   'est_idx_cov': 'est_idx',\n",
    "                                                   'est_pair_cov': 'est_pair'}),\n",
    "                             regional_bias])\n",
    "\n",
    "variances = regional_results['covar'].linalg.diagonal(\n",
    "                dims=['covar_pair_idx_1', 'covar_pair_idx_2'], offset=0,\n",
    "            )\n",
    "covariances = regional_results['covar'].linalg.diagonal(\n",
    "                dims=['covar_pair_idx_1', 'covar_pair_idx_2'], offset=1,\n",
    "              ).squeeze()\n",
    "regional_results['sigma_bias'] = np.sqrt(variances.sum(dim='covar_pair_idx_1') - 2 * covariances)\n",
    "\n",
    "norm_dist = stats.XrContinuousRV(norm,\n",
    "                                 loc=np.abs(regional_results['median_bias']),\n",
    "                                 scale=regional_results['sigma_bias'])\n",
    "# Set and name it as a DataArray as the attributes of the coordinates are not kept\n",
    "agreement_probability = norm_dist.cdf(0)\n",
    "agreement_probability.name = 'agreement_probability'\n",
    "\n",
    "# Merge to preserve coordinate attributes\n",
    "regional_results = xr.merge([regional_results, agreement_probability])\n",
    "regional_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bd950-8521-4c38-9a53-91cf539220b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = regional_results.agreement_probability.sel(season='All')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7.2, 3.5))\n",
    "\n",
    "x = np.arange(3)  # the label locations\n",
    "width = 1/(3 + len(regional_results.dataset_pair))  # the width of the bars\n",
    "multiplier = 2\n",
    "\n",
    "colors = plt.cm.tab20(np.linspace(0,1,15))\n",
    "for color, dataset_pair in zip(colors, regional_results.dataset_pair.data):\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data,\n",
    "                   width, label=dataset_pair.replace(' ', '-'), color=color, edgecolor='black')\n",
    "    _ = ax.errorbar(x + offset, data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data,\n",
    "                       yerr=np.abs(data.sel(dataset_pair=dataset_pair).quantile([0.16, 0.84], dim='est_idx').data\n",
    "                                   - data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data),\n",
    "                       linestyle='', color='black', capsize=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.plot(ax.get_xlim(), [0.05, 0.05], color='black', linestyle='dashed', linewidth=1)\n",
    "ax.plot(ax.get_xlim(), [0.16, 0.16], color='black', linestyle='dashed', linewidth=1)\n",
    "ax.set_ylabel('Agreement Probability', fontsize=14)\n",
    "xticks = regional_results.region_name.data\n",
    "xticks[2] = 'Upper Colorado\\nRiver Basin'\n",
    "ax.set_xticks(x + 9 * width, xticks)\n",
    "ax.legend(loc='upper left', ncols=3, columnspacing=1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.tick_params(axis=\"both\", direction=\"in\", right=True, top=True, labelsize=12)\n",
    "ax.set_yticks(np.linspace(0, 0.5, 3), np.linspace(0, 0.5, 3))\n",
    "ax.set_xlim(0, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('regional_agreement.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04732fa0-f1ca-48ac-a2f3-9a5b7709e765",
   "metadata": {},
   "source": [
    "# Figure 8\n",
    "\n",
    "Make bar plot of the agreement across regions separated out by season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2bcc4-0486-476d-b52d-39de1e6920a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=1, figsize=(7, 9), sharex=True, height_ratios=[0.4, 0.2, 0.2, 0.2])\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "letter = 'abcd'\n",
    "for i, (season, season_name) in enumerate({'DJF': 'winter', 'MAM': 'spring', 'JJA': 'summer', 'SON': 'fall'}.items()):\n",
    "    data = regional_results.agreement_probability.sel(season=season)\n",
    "\n",
    "    x = np.arange(3)  # the label locations\n",
    "    width = 1/(3 + len(regional_results.dataset_pair))  # the width of the bars\n",
    "    multiplier = 2\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 15))\n",
    "    for color, dataset_pair in zip(colors, regional_results.dataset_pair.data):\n",
    "        offset = width * multiplier\n",
    "        rects = ax[i].bar(x + offset, data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data,\n",
    "                       width, label=dataset_pair.replace(' ', '-'), color=color, edgecolor='black')\n",
    "        _ = ax[i].errorbar(x + offset, data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data,\n",
    "                           yerr=np.abs(data.sel(dataset_pair=dataset_pair).quantile([0.16, 0.84], dim='est_idx').data\n",
    "                                       - data.sel(dataset_pair=dataset_pair).median(dim='est_idx').data),\n",
    "                           linestyle='', color='black', capsize=3)\n",
    "        multiplier += 1\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax[i].set_title(f'({letter[i]}) ' + season_name,\n",
    "                    fontdict={'fontsize': 14})\n",
    "    ax[i].plot(ax[i].get_xlim(), [0.05, 0.05], color='black', linestyle='dashed', linewidth=1)\n",
    "    ax[i].plot(ax[i].get_xlim(), [0.16, 0.16], color='black', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    if i == 3:\n",
    "        xticks = regional_results.region_name.data\n",
    "        xticks[2] = 'Upper Colorado\\nRiver Basin'\n",
    "        ax[i].set_xticks(x + 9 * width, xticks)\n",
    "    if i == 0:\n",
    "        ax[i].legend(loc='upper left', ncols=3, columnspacing=1, fontsize=10)\n",
    "        ax[i].set_ylim(0, 1)\n",
    "    else:\n",
    "        ax[i].set_ylim(0, 0.5)\n",
    "    ax[i].set_yticks(np.linspace(0, 0.5, 3), np.linspace(0, 0.5, 3))\n",
    "    ax[i].tick_params(axis=\"both\", direction=\"in\", right=True, top=True, labelsize=12)\n",
    "    ax[i].set_xlim(0, 3)\n",
    "\n",
    "fig.supylabel('Agreement Probability', y=0.43, fontsize=16)\n",
    "plt.subplots_adjust(left=0.12, right=0.98, top=0.96, bottom=0.05, hspace=0.17)\n",
    "fig.savefig('regional_agreement_seasonal.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de187ea-6c1f-47c2-a203-cf5049c97dc8",
   "metadata": {},
   "source": [
    "Print more quoted stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e75a3-820d-4c81-af99-ee3dede711b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the region with the highest overall seasonal\n",
    "# agreement, the High Plains, the summer and fall have all data set pairs with probability values >0.1, indicating reasonable\n",
    "# agreement.\n",
    "regional_results['agreement_probability'].median(dim='est_idx').sel(region_name='High Plains Aquifer', season=['JJA', 'SON']) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f12b9-f695-4788-b9eb-49abe0b4991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the Central Valley shows similar trends in probabilities in the spring and fall as the UCRB, with 1â€“3\n",
    "# data set pairs having values below 0.05. Yet, when looking at the winter and summer, only a few data sets have probabilities\n",
    "# >0.16, with the majority of data set pairs having extremely low agreement (<0.015).\n",
    "# This is especially true in the summer, where 8 pairs (SSBEop or WBET paired with GLEAM, ERA5, NLDAS, or TerraClimte) have median probabilities <0.005\n",
    "print((regional_results['agreement_probability'].median(dim='est_idx').sel(region_name='Central Valley', season=['DJF']) < 0.015).sum())\n",
    "print((regional_results['agreement_probability'].median(dim='est_idx').sel(region_name='Central Valley', season=['JJA']) < 0.005).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
